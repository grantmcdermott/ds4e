<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>4 Parallel programming | Data Science for Economists and Other Animals</title>
<meta name="author" content="Grant McDermott and Ed Rubin">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.5.1/tabs.js"></script><script src="libs/bs3compat-0.2.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-76482472-1"></script><script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-76482472-1');
  </script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Data Science for Economists and Other Animals</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li class="book-part">Programming</li>
<li><a class="" href="funcs-intro.html"><span class="header-section-number">2</span> Functions: Introductory concepts</a></li>
<li><a class="" href="funcs-adv.html"><span class="header-section-number">3</span> Functions: Advanced concepts</a></li>
<li><a class="active" href="parallel.html"><span class="header-section-number">4</span> Parallel programming</a></li>
<li class="book-part">Analysis</li>
<li><a class="" href="spatial-analysis.html"><span class="header-section-number">5</span> Spatial analysis</a></li>
<li class="book-part">Cloud</li>
<li><a class="" href="gce-i.html"><span class="header-section-number">6</span> Google Compute Engine (I)</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/grantmcdermott/ds4e">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="parallel" class="section level1">
<h1>
<span class="header-section-number">4</span> Parallel programming<a class="anchor" aria-label="anchor" href="#parallel"><i class="fas fa-link"></i></a>
</h1>
<div id="software-requirements-2" class="section level2">
<h2>
<span class="header-section-number">4.1</span> Software requirements<a class="anchor" aria-label="anchor" href="#software-requirements-2"><i class="fas fa-link"></i></a>
</h2>
<div id="r-packages-2" class="section level3">
<h3>
<span class="header-section-number">4.1.1</span> R packages<a class="anchor" aria-label="anchor" href="#r-packages-2"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>New: <strong>parallel</strong>, <strong>future</strong>, <strong>future.apply</strong>, <strong>furrr</strong>, <strong>RhpcBLASctl</strong>, <strong>tictoc</strong>
</li>
<li>Already used: <strong>tidyverse</strong>, <strong>data.table</strong>, <strong>pbapply</strong>, <strong>memoise</strong>, <strong>here</strong>, <strong>hrbrthemes</strong>
</li>
</ul>
<p>The code chunk below will install (if necessary) and load all of these packages for you. Note that the <strong>parallel</strong> package is bundled together with the base R installation and should already be on your system. We’re also going to call the <code><a href="https://rdrr.io/pkg/future/man/plan.html">future::plan()</a></code> function and set the resolution to “multisession”. Don’t worry what this means right now; all with be explained in due course. For the moment, simply think of it as a convenient way to set our desired parallel programming behaviour for the rest of this chapter.</p>
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">## Load and install the packages that we'll be using in this chapter</span>
<span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">require</a></span><span class="op">(</span><span class="st"><a href="https://github.com/trinker/pacman">"pacman"</a></span><span class="op">)</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html">install.packages</a></span><span class="op">(</span><span class="st">"pacman"</span><span class="op">)</span>
<span class="fu">pacman</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/pacman/man/p_load.html">p_load</a></span><span class="op">(</span><span class="va">tictoc</span>, <span class="va">parallel</span>, <span class="va">pbapply</span>, <span class="va">future</span>, <span class="va">future.apply</span>, <span class="va">tidyverse</span>, 
               <span class="va">hrbrthemes</span>, <span class="va">furrr</span>, <span class="va">RhpcBLASctl</span>, <span class="va">memoise</span>, <span class="va">here</span><span class="op">)</span>
<span class="co">## ggplot2 plotting theme (optional)</span>
<span class="fu">theme_set</span><span class="op">(</span><span class="fu">hrbrthemes</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/hrbrthemes/man/theme_ipsum.html">theme_ipsum</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span>

<span class="co">## Set future::plan() resolution strategy</span>
<span class="fu">plan</span><span class="op">(</span><span class="va">multisession</span><span class="op">)</span></code></pre></div>
</div>
</div>
<div id="prologue" class="section level2">
<h2>
<span class="header-section-number">4.2</span> Prologue<a class="anchor" aria-label="anchor" href="#prologue"><i class="fas fa-link"></i></a>
</h2>
<p>Parallel programming is a big and complex topic, with many potential pitfalls. However, software innovations and some amazing new(ish) packages have made it <em>much</em> easier and safer to program in parallel in R.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;To clarify: R has provided excellent support for parallel programming for over a decade. But there’s no question in our minds that the barriers to entry have recently been lowered.&lt;/p&gt;"><sup>17</sup></a> With that in mind, we’re going to structure this chapter back-to-front. In particular, we’ll start with some motivating examples. Our primary goal is to demonstrate both the ease and immediate payoff of “going parallel”. Only after convincing you of these facts will we get into some of the technical details that were abstracted away behind the scenes. The latter part of the chapter will go over parallel programming in more general terms (i.e. not R-specific) and highlight potential pitfalls that you should be aware of.</p>
<p>Ready? Let’s go.</p>
</div>
<div id="example-1-slow_square" class="section level2">
<h2>
<span class="header-section-number">4.3</span> Example 1: slow_square<a class="anchor" aria-label="anchor" href="#example-1-slow_square"><i class="fas fa-link"></i></a>
</h2>
<p>Our first motivating example is going to involve the same <code>slow_square()</code> function that we created in Section <a href="funcs-adv.html#caching-memoisation">3.4</a>:</p>
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">## Emulate slow function</span>
<span class="va">slow_square</span> <span class="op">=</span> 
  <span class="kw">function</span><span class="op">(</span><span class="va">x</span> <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">{</span>
    <span class="va">x_sq</span> <span class="op">=</span> <span class="va">x</span><span class="op">^</span><span class="fl">2</span> 
    <span class="va">d</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>value <span class="op">=</span> <span class="va">x</span>, value_squared <span class="op">=</span> <span class="va">x_sq</span><span class="op">)</span>
    <span class="fu"><a href="https://rdrr.io/r/base/Sys.sleep.html">Sys.sleep</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span>
    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">d</span><span class="op">)</span>
    <span class="op">}</span></code></pre></div>
<p>Let’s iterate over this function using the standard <code><a href="https://rdrr.io/r/base/lapply.html">lapply()</a></code> method that we’re all familiar with by now. Note that this iteration will be executed in <em>serial</em>. We’ll use the <strong>tictoc</strong> package (<a href="https://cran.r-project.org/web/packages/tictoc/">link</a>) to record timing.</p>
<div class="sourceCode" id="cb83"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># library(tictoc) ## Already loaded</span>

<span class="fu">tic</span><span class="op">(</span><span class="op">)</span>
<span class="va">serial_ex</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">12</span>, <span class="va">slow_square</span><span class="op">)</span>
<span class="fu">toc</span><span class="op">(</span>log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="co">#&gt; 24.038 sec elapsed</span></code></pre></div>
<p>As expected, the iteration took about 24 seconds to run because of the enforced break after every sequential iteration (i.e. <code><a href="https://rdrr.io/r/base/Sys.sleep.html">Sys.sleep(2)</a></code>). On the other hand, this means that we can easily speed things up by iterating in <em>parallel</em>.</p>
<p>Before continuing, it’s worth pointing out that our ability to go parallel hinges on the number of CPU cores available to us. The simplest way to obtain this information from R is with the <code><a href="https://rdrr.io/r/parallel/detectCores.html">parallel::detectCores()</a></code> function:</p>
<div class="sourceCode" id="cb84"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># future::availableCores() ## Another option</span>
<span class="fu">detectCores</span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; [1] 2</span></code></pre></div>
<p>So, we have 2 cores to play with on this machine.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;&lt;del&gt;A Dell Precision 5530 running Arch Linux, if you’re interested.&lt;/del&gt;A cloud-based virtual machine provided by GitHub, which what we use to build this book.&lt;/p&gt;"><sup>18</sup></a> Adjust expectations for you own system accordingly.</p>
<p>Okay, back to our example. We’re going to implement the parallel iteration using the <strong>future.apply</strong> package (<a href="https://cran.r-project.org/web/packages/future.apply/index.html">link</a>) — more on this later. Note that the parameters of the problem are otherwise unchanged.</p>
<div class="sourceCode" id="cb85"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># library(future.apply)  ## Already loaded</span>
<span class="co"># plan(multisession)     ## Already set above</span>

<span class="fu">tic</span><span class="op">(</span><span class="op">)</span>
<span class="va">future_ex</span> <span class="op">=</span> <span class="fu">future_lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">12</span>, <span class="va">slow_square</span><span class="op">)</span>
<span class="fu">toc</span><span class="op">(</span>log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="co">#&gt; 12.24 sec elapsed</span></code></pre></div>
<p>Look at at that: A 1× speedup! Even more impressively, consider how little the syntax changed. We just had to tell R that we wanted to implement the iteration in parallel (i.e. <code><strong>plan(multisession)</strong></code>) and slightly amend our <code><a href="https://rdrr.io/r/base/lapply.html">lapply()</a></code> call (i.e. <code><strong>future_</strong>lapply()</code>).</p>
<p>Let’s confirm that the output is the same.</p>
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/all.equal.html">all.equal</a></span><span class="op">(</span><span class="va">serial_ex</span>, <span class="va">future_ex</span><span class="op">)</span>
<span class="co">#&gt; [1] TRUE</span></code></pre></div>
<p>For those of you who prefer the <code><a href="https://purrr.tidyverse.org/reference/map.html">purrr::map()</a></code> family of functions for iteration and are feeling left out; don’t worry. The <strong>furrr</strong> package (<a href="https://davisvaughan.github.io/furrr/index.html">link</a>) has you covered. Once again, the syntax for these parallel functions will be very little changed from their serial versions. We simply have to tell R that we want to run things in parallel with <code>plan(multisession)</code> and then slightly amend our map call to <code><strong>future_</strong>map_df<strong>r</strong>()</code>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;In this particular case, the extra “r” at the end tells future to concatenate the data frames from each iteration by &lt;em&gt;rows&lt;/em&gt;.&lt;/p&gt;"><sup>19</sup></a></p>
<div class="sourceCode" id="cb87"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># library(furrr)      ## Already loaded</span>
<span class="co"># plan(multisession)  ## Already set above</span>

<span class="fu">tic</span><span class="op">(</span><span class="op">)</span>
<span class="va">furrr_ex</span> <span class="op">=</span> <span class="fu">future_map_dfr</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">12</span>, <span class="va">slow_square</span><span class="op">)</span>
<span class="fu">toc</span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; 12.24 sec elapsed</span></code></pre></div>
<p>How easy was that? We hardly had to change our original code and didn’t have to pay a cent for all that extra performance.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Not to flog a dead horse, but as we pointed out in the very &lt;a href="https://raw.githack.com/uo-ec607/lectures/master/01-intro/01-Intro.html#26"&gt;first lecture&lt;/a&gt; of this course: Have you seen the price of a &lt;a href="https://www.stata.com/statamp/"&gt;Stata/MP&lt;/a&gt; license recently? Not to mention the fact that you effectively pay &lt;em&gt;per&lt;/em&gt; core…&lt;/p&gt;'><sup>20</sup></a> Congratulate yourself on already being such an expert at parallel programming.</p>
<div class="inline-figure"><img src="pics/parallel/liz-high-five.gif" width="100%" style="display: block; margin: auto;"></div>
</div>
<div id="example-2-bootstrapping" class="section level2">
<h2>
<span class="header-section-number">4.4</span> Example 2: Bootstrapping<a class="anchor" aria-label="anchor" href="#example-2-bootstrapping"><i class="fas fa-link"></i></a>
</h2>
<p>Our second motivating example will involve a more realistic and slightly more computationally-intensive case: Bootstrapping coefficient values for hypothesis testing.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;It’s an aside, but we’re strongly of the opinion that simulation methods like bootstrapping provide a far more intuitive way to understand statistics. Don’t believe us? Watch John Rausser’s fantastic talk: “&lt;a href="https://www.youtube.com/watch?v=5Dnw46eC-0o"&gt;&lt;em&gt;Statistics Without The Agonizing Pain&lt;/em&gt;&lt;/a&gt;”. Seriously, watch it.&lt;/p&gt;'><sup>21</sup></a> We’ll also spend a bit more time talking about the packages we’re using and what they’re doing.</p>
<p>Start by creating a fake dataset (<code>d</code>), where we “know” that the relationship between our x and y variables. We’ll then specify a bootstrapping function (<code><a href="https://rstudio.github.io/bslib/reference/deprecated.html">bootstrap()</a></code>) that will draw a random sample of 1,000 observations from the fake dataset (with replacement), fit a regression, and then extract the coefficient on the x variable.</p>
<div class="sourceCode" id="cb88"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">## Set seed (for reproducibility)</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span>
<span class="co"># Set sample size</span>
<span class="va">n</span> <span class="op">=</span> <span class="fl">1e6</span>

<span class="co">## Generate a large data frame of fake data for a regression</span>
<span class="va">d</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>, e <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span>
<span class="co">## Outcome variable. Note that the x coefficient is 2.</span>
<span class="va">d</span><span class="op">$</span><span class="va">y</span> <span class="op">=</span> <span class="fl">3</span> <span class="op">+</span> <span class="fl">2</span><span class="op">*</span><span class="va">d</span><span class="op">$</span><span class="va">x</span> <span class="op">+</span> <span class="va">d</span><span class="op">$</span><span class="va">e</span>

<span class="co">## Function that draws a sample observations from the fake dataset, then runs a </span>
<span class="co">## regression and extracts the coefficient value on the x variable.</span>
<span class="va">bootstrap</span> <span class="op">=</span> 
  <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span>
  <span class="co">## Sample 1,000 rows of our fake dataset</span>
  <span class="va">dsamp</span> <span class="op">=</span> <span class="va">d</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample.int</a></span><span class="op">(</span><span class="fl">1e3</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>, <span class="op">]</span>
  <span class="co">## Run a regression on the sampled data, then extract the extract the x</span>
  <span class="co">## coefficient (should be around 2).</span>
  <span class="va">x_coef</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, <span class="va">dsamp</span><span class="op">)</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="st">'x'</span><span class="op">]</span>
  <span class="co">## Return value</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>sim <span class="op">=</span> <span class="va">i</span>, x_coef <span class="op">=</span> <span class="va">x_coef</span><span class="op">)</span><span class="op">)</span>
  <span class="op">}</span></code></pre></div>
<div id="serial-implementation-for-comparison" class="section level3">
<h3>
<span class="header-section-number">4.4.1</span> Serial implementation (for comparison)<a class="anchor" aria-label="anchor" href="#serial-implementation-for-comparison"><i class="fas fa-link"></i></a>
</h3>
<p>Let’s implement the bootstrap procedure in serial to get a benchmark for comparison. Note that we’ll repeat the function (i.e. simulation) 10,000 times to build up a decent estimate of the parameter distribution; which, again, we know should be around 2.</p>
<div class="sourceCode" id="cb89"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123L</span><span class="op">)</span> <span class="co">## Optional to ensure that the results are the same</span>

<span class="co">## 10,000-iteration simulation</span>
<span class="fu">tic</span><span class="op">(</span><span class="op">)</span>
<span class="va">sim_serial</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">1e4</span>, <span class="va">bootstrap</span><span class="op">)</span>
<span class="fu">toc</span><span class="op">(</span>log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="co">#&gt; 16.871 sec elapsed</span></code></pre></div>
<p>So that took about 24 seconds on this system. Not a huge pain, but let’s see if we can do better by switching to a parallel (multicore) implementation. For the record, though here is a screenshot of Grant’s system monitor, showing that only one core was being used during this serial version.</p>
<blockquote>
<p><strong>Note:</strong> Grant’s local computer has 12 cores, which may differ from the cloud-based virtual machine that was used to build the online version of the book that you are reading.</p>
</blockquote>
<div class="inline-figure"><img src="pics/parallel/serial.png" width="100%" style="display: block; margin: auto;"></div>
</div>
<div id="parallel-implemention-using-the-future-ecosystem" class="section level3">
<h3>
<span class="header-section-number">4.4.2</span> Parallel implemention using the <strong>future</strong> ecosystem<a class="anchor" aria-label="anchor" href="#parallel-implemention-using-the-future-ecosystem"><i class="fas fa-link"></i></a>
</h3>
<p>All of the parallel programming that we’ve been doing so far is built on top of <a href="https://twitter.com/henrikbengtsson">Henrik Bengtsson’s</a> amazing <strong>future</strong> package (<a href="https://cran.r-project.org/web/packages/future/index.html">link</a>). A “future” is basically a very flexible way of evaluating code and output. Among other things, this allows you to switch effortlessly between evaluating code in <em>serial</em> or <em>asynchronously</em> (i.e. in parallel). You simply have to set your resolution <em>plan</em> — “sequential”, “multisession”, “cluster”, etc. — and let future handle the implementation for you.</p>
<p>Here’s Henrik <a href="https://cran.r-project.org/web/packages/future/vignettes/future-1-overview.html">describing</a> the core idea in more technical terms:</p>
<blockquote>
<p>In programming, a <em>future</em> is an abstraction for a <em>value</em> that may be available at some point in the future. The state of a future can either be unresolved or resolved… Exactly how and when futures are resolved depends on what strategy is used to evaluate them. For instance, a future can be resolved using a sequential strategy, which means it is resolved in the current R session. Other strategies may be to resolve futures asynchronously, for instance, by evaluating expressions in parallel on the current machine or concurrently on a compute cluster.</p>
</blockquote>
<p>As we’ve tried to emphasise, <strong>future</strong> is relatively new on the scene. It is certainly not the first or only way to implement parallel processes in R. However, <strong>future</strong> provides a simple and unified framework that makes it (in our view) the preeminent choice. What’s more, the same commands that we use here will carry over very neatly to more complicated settings involving high-performance computing clusters. We’ll experience this first hand when we get to the big data section of the book.</p>
<p>You’ve probably also noted that keep referring to the “future ecosystem”. This is because <strong>future</strong> provides the framework for other packages to implement parallel versions of their functions. The two that we’ll focus on here are:</p>
<ol style="list-style-type: decimal">
<li>the <strong>future.apply</strong> package (<a href="https://cran.r-project.org/web/packages/future.apply/index.html">link</a>), also by Henrik, and</li>
<li>the <strong>furrr</strong> package (<a href="https://davisvaughan.github.io/furrr/index.html">link</a>), an implementation for <strong>purrr</strong> by <a href="https://twitter.com/dvaughan32">Davis Vaughan</a>.</li>
</ol>
<p>In both cases, we start by setting the plan for resolving the future evaluation (here: <code>plan(multisession)</code>). We then call our functions — which involve minor modifications of their serial equivalents — and let future magic take care of everything else.</p>
<div id="future.apply" class="section level4">
<h4>
<span class="header-section-number">4.4.2.1</span> future.apply<a class="anchor" aria-label="anchor" href="#future.apply"><i class="fas fa-link"></i></a>
</h4>
<p>Here’s the <code><a href="https://rdrr.io/pkg/future.apply/man/future_lapply.html">future.apply::future_lapply()</a></code> parallel implementation. Note that we’re adding the <code>future.seed=123L</code> option to ensure that the results are the same. While not strictly necessary, it’s always a good idea to set a random seed with simulations for the sake of reproducibility.</p>
<div class="sourceCode" id="cb90"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># library(future.apply)  ## Already loaded</span>
<span class="co"># plan(multisession)     ## Already set above</span>

<span class="co">## 10,000-iteration simulation</span>
<span class="fu">tic</span><span class="op">(</span><span class="op">)</span>
<span class="va">sim_future</span> <span class="op">=</span> <span class="fu">future_lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">1e4</span>, <span class="va">bootstrap</span>, future.seed<span class="op">=</span><span class="fl">123L</span><span class="op">)</span>
<span class="fu">toc</span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; 9.524 sec elapsed</span></code></pre></div>
<p>Remember from our previous programming chapters that <code>lapply</code> returns a list (in this case: a list of 10,000 single row data frames). So we would have have to bind these elements together in a single, large data frame if that’s what we wanted. For example,</p>
<div class="sourceCode" id="cb91"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">## Bind into single data frame</span>
<span class="va">sim_future</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/do.call.html">do.call</a></span><span class="op">(</span><span class="st">"rbind"</span>, <span class="va">sim_future</span><span class="op">)</span>
<span class="co"># sim_future = dplyr::bind_rows("rbind", sim_future)       ## Another option</span>
<span class="co"># sim_future = data.table::rbindlist("rbind", sim_future)  ## Another option</span>

<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">sim_future</span><span class="op">)</span>
<span class="co">#&gt;    sim   x_coef</span>
<span class="co">#&gt; x    1 2.038696</span>
<span class="co">#&gt; x1   2 1.988938</span>
<span class="co">#&gt; x2   3 1.970760</span>
<span class="co">#&gt; x3   4 1.968304</span>
<span class="co">#&gt; x4   5 2.011383</span>
<span class="co">#&gt; x5   6 2.001061</span></code></pre></div>
</div>
<div id="furrr" class="section level4">
<h4>
<span class="header-section-number">4.4.2.2</span> furrr<a class="anchor" aria-label="anchor" href="#furrr"><i class="fas fa-link"></i></a>
</h4>
<p>And here’s the <code><a href="https://rdrr.io/pkg/furrr/man/future_map.html">furrr::future_map_dfr()</a></code> implementation. Similar to the above, note that we’re only adding the <code>.options=future_options(seed=123L)</code> option to ensure that the output is exactly the same.</p>
<div class="sourceCode" id="cb92"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># library(furrr)      ## Already loaded</span>
<span class="co"># plan(multisession)  ## Already set above</span>

<span class="co">## 10,000-iteration simulation</span>
<span class="fu">tic</span><span class="op">(</span><span class="op">)</span>
<span class="va">sim_furrr</span> <span class="op">=</span> <span class="fu">future_map_dfr</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">1e4</span>, <span class="va">bootstrap</span>, .options <span class="op">=</span> <span class="fu">furrr_options</span><span class="op">(</span>seed<span class="op">=</span><span class="fl">123L</span><span class="op">)</span><span class="op">)</span>
<span class="fu">toc</span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; 9.378 sec elapsed</span>

<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">sim_furrr</span><span class="op">)</span>
<span class="co">#&gt;       sim   x_coef</span>
<span class="co">#&gt; x...1   1 2.038696</span>
<span class="co">#&gt; x...2   2 1.988938</span>
<span class="co">#&gt; x...3   3 1.970760</span>
<span class="co">#&gt; x...4   4 1.968304</span>
<span class="co">#&gt; x...5   5 2.011383</span>
<span class="co">#&gt; x...6   6 2.001061</span></code></pre></div>
</div>
</div>
<div id="results" class="section level3">
<h3>
<span class="header-section-number">4.4.3</span> Results<a class="anchor" aria-label="anchor" href="#results"><i class="fas fa-link"></i></a>
</h3>
<p>As expected, we dramatically cut down on total computation time by going parallel. Depending on your system, you may note that the parallel improvements for this example didn’t scale linearly with the number of cores. The reason has to do with the <em>overhead</em> of running the parallel implementations — a topic that we will revisit in Section <a href="parallel.html#when-should-i-go-parallel">4.7.1</a>. Once again and for the record, here is a screenshot of Grant’s laptop, showing that all cores were being used during these parallel implementations.</p>
<blockquote>
<p><strong>Note:</strong> Grant’s local computer has 12 cores, which may differ from the cloud-based virtual machine that was used to build the online version of the book that you are reading.</p>
</blockquote>
<div class="inline-figure"><img src="pics/parallel/parallel.png" width="100%" style="display: block; margin: auto;"></div>
<p>While it wasn’t exactly hard work, let’s reward ourselves by displaying the results from our bootstrapping simulation in the form of a nice plot. We’ll use the <code>sim_furrr</code> results data frame for this, although it doesn’t matter since they’re all the same thanks to our our random seed. As you can see, the estimated coefficient values are tightly clustered around our simulated mean of 2.</p>
<div class="sourceCode" id="cb93"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">sim_furrr</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">x_coef</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_density</span><span class="op">(</span>col<span class="op">=</span><span class="cn">NA</span>, fill<span class="op">=</span><span class="st">"gray25"</span>, alpha<span class="op">=</span><span class="fl">0.3</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_vline</span><span class="op">(</span>xintercept<span class="op">=</span><span class="fl">2</span>, col<span class="op">=</span><span class="st">"red"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">labs</span><span class="op">(</span>
    title <span class="op">=</span> <span class="st">"Bootstrapping example"</span>,
    x<span class="op">=</span><span class="st">"Coefficient values"</span>, y<span class="op">=</span><span class="st">"Density"</span>,
    caption <span class="op">=</span> <span class="st">"Notes: Density based on 10,000 draws with sample size of 10,000 each."</span>
    <span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="parallel_files/figure-html/x_coef-1.png" width="672" style="display: block; margin: auto;"></div>
</div>
<div id="other-parallel-options" class="section level3">
<h3>
<span class="header-section-number">4.4.4</span> Other parallel options<a class="anchor" aria-label="anchor" href="#other-parallel-options"><i class="fas fa-link"></i></a>
</h3>
<p>Futures are not the only game in town for parallel programming in R. For example, we’ll talk more about the <code>mclapply</code> function that comes bundled with base R in Section <a href="parallel.html#forking-vs-sockets">4.5.3</a> below. However, one particular option that we want to mention very briefly is the <strong>pbapply</strong> package (<a href="https://github.com/psolymos/pbapply">link</a>). As we saw in Section <a href="funcs-intro.html#aside-progress-bars">2.5.3.2.1</a>, this package provides a lightweight wrapper for the <code>*apply</code> family of functions by adding a progress bar. However, the package also adds a very convenient option for multicore implementation. You basically just have to add <code>cl=CORES</code> to the call. While it doesn’t rely on futures, <strong>pbapply</strong> also takes care of all the OS-specific overhead for you. See <a href="http://peter.solymos.org/code/2016/09/11/what-is-the-cost-of-a-progress-bar-in-r.html">here</a> for an interesting discussion on what’s happening behind the scenes.</p>
<p>You will need to run this next chunk interactively to see the progress bar.</p>
<div class="sourceCode" id="cb94"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span> <span class="co">## Optional to ensure results are exactly the same.</span>

<span class="co"># library(pbapply) ## Already loaded</span>

<span class="co">## 10,000-iteration simulation</span>
<span class="fu">tic</span><span class="op">(</span><span class="op">)</span>
<span class="va">sim_pblapply</span> <span class="op">=</span> <span class="fu">pblapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">1e4</span>, <span class="va">bootstrap</span>, cl <span class="op">=</span> <span class="fu">parallel</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/parallel/detectCores.html">detectCores</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span>
<span class="fu">toc</span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; 9.403 sec elapsed</span></code></pre></div>
<blockquote>
<p><strong>Aside:</strong> On the subject of progress bars, check out the <strong>progressr</strong> package (<a href="https://github.com/HenrikBengtsson/progressr">link</a>) for a unified framework that works with all kinds of functions and (a)syncronous backends.</p>
</blockquote>
</div>
</div>
<div id="general-parallel-programming-topics" class="section level2">
<h2>
<span class="header-section-number">4.5</span> General parallel programming topics<a class="anchor" aria-label="anchor" href="#general-parallel-programming-topics"><i class="fas fa-link"></i></a>
</h2>
<p>Motivating examples out of the way, let’s take a look underneath the hood. We want to emphasise that this section is more “good to know” than “need to know”. Even if you take nothing else away from rest of this chapter, you are already well placed to begin implementing parallel functions at a much larger scale.</p>
<p>And yet… while you don’t <em>need</em> to know the next section in order to program in parallel in R, getting a solid grasp of the basics is valuable. It will give you a better understanding of how parallel programming works in general and help you to appreciate how much <strong>future</strong> and co. are doing behind the scenes for you. It will also help you to understand why the same code runs faster on some systems than others, and avoid some common pitfalls.</p>
<div id="terminology" class="section level3">
<h3>
<span class="header-section-number">4.5.1</span> Terminology<a class="anchor" aria-label="anchor" href="#terminology"><i class="fas fa-link"></i></a>
</h3>
<p>We’ll start by clearing up some terminology.</p>
<ul>
<li>
<strong>Socket:</strong> The physical connection on your computer that houses the processor. Most work and home computers — even very high-end ones — only have one socket and, thus, one processor. However, they can have multiple cores. Speaking of which…</li>
<li>
<strong>Core:</strong> The part of the processor that actually performs the computation. Back in the day, processors were limited to a single core. However, most modern processors now house multiple cores. Each of these cores can perform entirely separate and independent computational processes.</li>
<li>
<strong>Process:</strong> A single instance of a running task or program (R, Dropbox, etc). A single core can only run one process at a time. However, it may give the appearance of doing more than that by efficiently scheduling between them. Speaking of which…</li>
<li>
<strong>Thread:</strong> A component or subset of a process that can, <em>inter alia</em>, share memory and resources with other threads. We’ll return to this idea as it applies to <em>hyperthreading</em> in a few paragraphs.</li>
<li>
<strong>Cluster:</strong> A collection of objects that are capable of hosting cores. This could range from a single socket (on your home computer) to an array of servers (on a high-performance computing network).</li>
</ul>
<p>You may wondering where the much-referenced <strong>CPU</strong> (i.e. central processing unit) fits into all of this. Truth be told, the meaning of CPU has evolved with the advent of new technology like multicore processors. For the purposes of this book, we will use the following definition:</p>
<p><span class="math display">\[\text{No. of CPUs} = \text{No. of sockets} \times \text{No. of physcial cores} \times \text{No. of threads per core}\]</span></p>
<p>If nothing else, this is consistent with the way that Linux records information about CPU architecture via the <a href="https://linux.die.net/man/1/lscpu"><code>lscpu</code></a> shell command. For example, here is what Grant sees when he runs the command on his laptop:</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb95-1"><a href="parallel.html#cb95-1"></a><span class="ex">lscpu</span> <span class="kw">|</span> <span class="fu">grep</span> -E <span class="st">'^Thread|^Core|^Socket|^CPU\('</span>    <span class="co">## Only works on Linux</span></span>
<span id="cb95-2"><a href="parallel.html#cb95-2"></a><span class="co">#&gt; CPU(s):                          12</span></span>
<span id="cb95-3"><a href="parallel.html#cb95-3"></a><span class="co">#&gt; Thread(s) per core:              2</span></span>
<span id="cb95-4"><a href="parallel.html#cb95-4"></a><span class="co">#&gt; Core(s) per socket:              6</span></span>
<span id="cb95-5"><a href="parallel.html#cb95-5"></a><span class="co">#&gt; Socket(s):                       1</span></span></code></pre></div>
<p>Note that the headline “CPU(s)” number is the same that he would get from running <code><a href="https://rdrr.io/r/parallel/detectCores.html">parallel::detectCores()</a></code>, i.e. 12.</p>
</div>
<div id="a-bit-more-about-logical-cores-and-hyperthreading" class="section level3">
<h3>
<span class="header-section-number">4.5.2</span> A bit more about logical cores and hyperthreading<a class="anchor" aria-label="anchor" href="#a-bit-more-about-logical-cores-and-hyperthreading"><i class="fas fa-link"></i></a>
</h3>
<p>Logical cores extend or emulate the ability of physical cores to perform additional tasks. The most famous example is Intel’s <a href="https://en.wikipedia.org/wiki/Hyper-threading"><strong>hyperthreading</strong></a> technology, which allows a single core to switch very rapidly between two different tasks. This mimics the appearance and performance (albeit to a lesser extent) of an extra physical core. You may find <a href="https://www.youtube.com/watch?v=mSZpDF-zUoI&amp;">this YouTube video</a> helpful for understanding the difference in more depth, including a nice analogy involving airport security lines.</p>
<p>Taking a step back, you don’t have to worry too much about the difference between physical and logical (hyperthreaded) cores for the purpose of this chapter. R doesn’t care whether you run a function on a physical core or a logical one. Both will work equally well. (Okay, the latter will be a little slower.) Still, if you are interested in determining the number of physical cores versus logical cores on your system, then there are several ways to this from R. For example, you can use the <strong>RhpcBLASctl</strong> package (<a href="https://cran.r-project.org/web/packages/RhpcBLASctl/index.html">link</a>).</p>
<div class="sourceCode" id="cb96"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># library(RhpcBLASctl) ## Already loaded</span>

<span class="fu">get_num_procs</span><span class="op">(</span><span class="op">)</span> <span class="co">## No. of all cores (including logical/hyperthreaded)</span>
<span class="co">#&gt; [1] 2</span>
<span class="fu">get_num_cores</span><span class="op">(</span><span class="op">)</span> <span class="co">## No. of physical cores only</span>
<span class="co">#&gt; [1] 2</span></code></pre></div>
</div>
<div id="forking-vs-sockets" class="section level3">
<h3>
<span class="header-section-number">4.5.3</span> Forking vs Sockets<a class="anchor" aria-label="anchor" href="#forking-vs-sockets"><i class="fas fa-link"></i></a>
</h3>
<p>As we keep saying, it’s now incredibly easy to run parallel programs in R. The truth is that it has actually been easy to do so for a long time, but the implementation used to vary by operating system. In particular, simple parallel implementations that worked perfectly well on Linux or Mac didn’t work on Windows (which required a lot more overhead). For example, take a look at the <a href="https://stat.ethz.ch/R-manual/R-devel/library/parallel/html/mclapply.html">help documentation</a> for the <code><a href="https://rdrr.io/r/parallel/mclapply.html">parallel::mclapply()</a></code> function, which has been around since 2011. If you did so, you would see a warning that <code>mclapply()</code> <em>“relies on forking and hence is not available on Windows”</em>.</p>
<p>Now, we clearly didn’t encounter any OS-specific problems when we ran the parallel versions of our motivating examples above. The same code worked for everyone, including anyone using Windows. <del><em>Loud booing.</em></del> What was happening behind the scenes is that the <strong>future</strong> packages automatically handled any complications for us. The parallel functions were being executed in a way that was optimized for each person’s OS and R environment.</p>
<p>But what is “forking” and why does it matter what OS we are using anyway? Those are good questions that relate to the method of parallelization (i.e. type of cluster) that your system supports. The short version is that there are basically two ways that code can be parallelized:</p>
<ul>
<li>
<strong>Forking</strong> works by cloning your entire R environment to each separate core. This includes your data, loaded packages, functions, and any other objects in your current session. This is very efficient because you don’t have to worry about reproducing your “master” environment in each “worker” node. Everything is already linked, which means that you aren’t duplicating objects in memory. However, forking is not supported on Windows and can also cause problems in an IDE or GUI like RStudio.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;The reason is that shared GUI elements are being shared across child processes. (See the “GUI/embedded environments” section &lt;a href="https://stat.ethz.ch/R-manual/R-devel/library/parallel/html/mcfork.html"&gt;here&lt;/a&gt;.) Combined with the fact that the shared memory is &lt;em&gt;read only&lt;/em&gt;, there is a risk of “cross-contamination” where any modification made to a shared object by one of the worker nodes can propagates to the system.&lt;/p&gt;'><sup>22</sup></a>
</li>
<li>
<strong>Parallel sockets</strong> (aka “PSOCKs”) work by launching a new R session in each core. This means that your master environment has to be copied over and instantiated separately in each parallel node. This requires greater overhead and causes everything to run slower, since objects will be duplicated across each core. Technically, a PSOCK works by establishing a network (e.g. as if you were connected to a remote cluster), but everything is self-contained on your computer. This approach can be implemented on every system, including Windows, and doesn’t create problems for IDEs like RStudio.</li>
</ul>
<p>We’ve summarised the differences between the two approaches in the table below.</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="left">Forking</th>
<th align="left">PSOCKs</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">✓ Fast and memory efficient.</td>
<td align="left">× Slower and more memory-intensive (than forking).</td>
</tr>
<tr class="even">
<td align="left">× Only available for Unix-based systems.</td>
<td align="left">✓ Works on every operating system, incl. Windows.</td>
</tr>
<tr class="odd">
<td align="left">× Potentially unstable in an IDE like RStudio.</td>
<td align="left">✓ Fine to use in an IDE like RStudio.</td>
</tr>
</tbody>
</table></div>
<p>The general rule of thumb is PSOCKs are safer and more universal than forking. This makes them a good default, even if they do come with a potential performance/memory penalty. And, indeed, this is exactly what we did by selecting the <strong><code>plan(multisession)</code></strong> resolution strategy. So, now you know where that choice came from.</p>
<p>At the same time, however, we recommend that you consider forking if it is available to you (i.e. you’re on Linux or Mac) and you want to maximize performance. In these cases, the solution requires two simple tweaks:</p>
<ol style="list-style-type: decimal">
<li>Change your resolution plan to <strong><code>plan(multicore)</code></strong>, and</li>
<li>Run your R script from the terminal using, say, <code>$ Rscript -e 'rmarkdown::render("mydoc.Rmd", "all")'</code> or <code>$ Rscript myfile.R</code>.</li>
</ol>
<p>Here’s a very simple illustration, using the same setup as Example 1 from earlier.</p>
<div class="sourceCode" id="cb97"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">plan</span><span class="op">(</span><span class="va">multicore</span><span class="op">)</span> <span class="co">## NB: Only works on Unix!</span>

<span class="fu">tic</span><span class="op">(</span><span class="op">)</span>
<span class="va">future_ex_mc</span> <span class="op">=</span> <span class="fu">future_lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">12</span>, <span class="va">slow_square</span><span class="op">)</span>
<span class="fu">toc</span><span class="op">(</span>log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="co">#&gt; 12.128 sec elapsed</span></code></pre></div>
<p>Here we see that invoking the forked parallel backend (i.e. <code>plan(multicore)</code>) is indeed faster than the PSOCK equivalent (i.e. <code>plan(multisession)</code>). But remember: <em>Caveat emptor</em>.</p>
<p>For more on forks vs PSOCKS, take a look at the <a href="https://github.com/HenrikBengtsson/future#asynchronous-futures">relevant section</a> of the <strong>future</strong> README.</p>
</div>
</div>
<div id="explicit-vs-implicit-parallelization" class="section level2">
<h2>
<span class="header-section-number">4.6</span> Explicit vs implicit parallelization<a class="anchor" aria-label="anchor" href="#explicit-vs-implicit-parallelization"><i class="fas fa-link"></i></a>
</h2>
<p>Thus far we have only been concerned with <em>explicit</em> parallelization. As in, we explicitly told R to run a particular set of commands in parallel. But there is another form of <em>implicit</em> parallelization that is equally important to be aware of. In this case, certain low-level functions and operations are automatically run in parallel regardless of whether we “told” R to do so or not. Implicit parallelization can make a big difference to performance, but is not the default behaviour in R. So it has to enabled first (for example, by the package that you’re using). Moreover, combining explicit and implicit parallelization can cause problems if you don’t take certain precautions. Let’s take a look at where implicit parallelization enters the fray.</p>
<div id="blaslapack" class="section level3">
<h3>
<span class="header-section-number">4.6.1</span> BLAS/LAPACK<a class="anchor" aria-label="anchor" href="#blaslapack"><i class="fas fa-link"></i></a>
</h3>
<p>Did you ever wonder how R and other programming languages perform their calculations? For example, how does R actually do things like vector addition, or scalar and matrix multiplication? The answer is <a href="http://www.netlib.org/blas/"><strong>BLAS</strong></a> (<strong>B</strong>asic <strong>L</strong>inear <strong>A</strong>lgebra <strong>S</strong>uprograms). BLAS are a collection of low-level routines that provide standard building blocks for performing basic vector and matrix operations. These routines are then incorporated in related libraries like <a href="http://www.netlib.org/lapack/"><strong>LAPACK</strong></a> (<strong>L</strong>inear <strong>A</strong>lgebra <strong>Pack</strong>age), which provide their own routines for solving systems of linear equations and linear least squares, calculating eigenvalues, etc. In other words, BLAS and LAPACK provide the linear algebra framework that supports virtually all of statistical and computational programming</p>
<p>R ships with its own BLAS/LAPACK libraries by default. These libraries place a premium on stability (e.g. common user experience across operating systems). While the default works well enough, you can get <em>significant</em> speedups by switching to more optimized libraries such as the <a href="https://software.intel.com/en-us/mkl">Intel Math Kernel Library (MKL)</a> or <a href="https://www.openblas.net/">OpenBLAS</a>. Among other things, these optimized BLAS libraries support multi-threading. So now you are using all your available computer power to, say, solve a matrix.</p>
<p>You can use the <code><a href="https://rdrr.io/r/utils/sessionInfo.html">sessionInfo()</a></code> command to see which BLAS/LAPACK library you are using.</p>
</div>
<div id="beware-resource-competition" class="section level3">
<h3>
<span class="header-section-number">4.6.2</span> Beware resource competition<a class="anchor" aria-label="anchor" href="#beware-resource-competition"><i class="fas fa-link"></i></a>
</h3>
<p>While this all sounds great — and we certainly recommend taking a look at MKL or OpenBLAS — there is a potential downside. In particular, you risk competing with yourself for computational resources (i.e. memory) if you mix explicit and implicit parallel calls. For instance, if you run explicit multicore functions from within R on a system that has been configured with an optimized BLAS. As <a href="http://dirk.eddelbuettel.com/">Dirk Eddelbuettel</a> succinctly puts it in <a href="https://stackoverflow.com/a/18291826">this Stack Overflow thread</a>:</p>
<blockquote>
<p>There is one situation you want to avoid: (1) spreading a task over all <em>N</em> cores and (2) having each core work on the task using something like OpenBLAS or MKL with all cores. Because now you have an <em>N</em> by <em>N</em> contention: each of the N task wants to farm its linear algebra work out to all <em>N</em> cores.</p>
</blockquote>
<p>Now, we should emphasise that nested parallelism has rarely caused major problems in our own experience. We both use optimized BLAS libraries, run explicit parallel calls all the time in our R scripts, and have only occasionally run into issues. Moreover, when these slowdowns have occurred, the impact appears to have been relatively modest.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;The major cost appears to be the unnecessary duplication of objects in memory.&lt;/p&gt;"><sup>23</sup></a> Still, we have heard of cases where the effect is much more dramatic (e.g. <a href="https://stat.ethz.ch/pipermail/r-sig-hpc/2014-February/001846.html">here</a>). Consider yourselves forewarned so as to be forearmed.</p>
<p>With that admonishment in mind, thankfully there is a an easy and relatively costless solution: Simply turn off BLAS multithreading. It turns out this has a negligible impact on performance, since most of the gains from optimized BLAS are actually coming from improved math vectorisation, not multi-threading.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Detailed discussion here: &lt;a href="https://blog.revolutionanalytics.com/2015/10/edge-cases-in-using-the-intel-mkl-and-parallel-programming.html" class="uri"&gt;https://blog.revolutionanalytics.com/2015/10/edge-cases-in-using-the-intel-mkl-and-parallel-programming.html&lt;/a&gt;&lt;/p&gt;'><sup>24</sup></a> You can turn off BLAS multithreading for the current R session with the <code><a href="https://rdrr.io/pkg/RhpcBLASctl/man/RhpcBLASctl-package.html">RhpcBLASctl::blas_set_num_threads()</a></code> function. For example, we sometimes include the following line at the top of an R script:</p>
<div class="sourceCode" id="cb98"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># blas_get_num_procs() ## If you want to find the existing number of BLAS threads</span>
<span class="fu">RhpcBLASctl</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/RhpcBLASctl/man/RhpcBLASctl-package.html">blas_set_num_threads</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span> <span class="co">## Set BLAS threads to 1 (i.e. turn off multithreading)</span></code></pre></div>
<p>Since this is only in effect for the current R session, BLAS multithreading will be restored when we restart R.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;We could also reinstate the original behaviour in the same session by running &lt;code&gt;blas_set_num_threads(parallel::detectCores())&lt;/code&gt;.&lt;/p&gt;"><sup>25</sup></a> You can turn off BLAS multithreading as the default mode by setting an appropriate environment variable. We’ll see an example of this in Section <a href="gce-i.html#mkl">6.6.1</a>.</p>
</div>
<div id="library-source-code" class="section level3">
<h3>
<span class="header-section-number">4.6.3</span> Library source code<a class="anchor" aria-label="anchor" href="#library-source-code"><i class="fas fa-link"></i></a>
</h3>
<p>Implicit parallelization is automatically invoked by many of the external libraries that we use in R. The good news is that package developers normally take pains to avoid potential resource competition. For instance, consider the message that <strong>data.table</strong> greets us with at load time.</p>
<div class="sourceCode" id="cb99"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-datatable.com">data.table</a></span>, warn.conflicts <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></code></pre></div>
<pre><code>#&gt; data.table 1.14.0 using 6 threads (see ?getDTthreads).  Latest news: r-datatable.com</code></pre>
<p>If you follow the suggestion and look at the <code><a href="https://Rdatatable.gitlab.io/data.table/reference/openmp-utils.html">?getDTthreads</a></code> help documentation, you’ll find an informative (and reassuring) discussion of its approach here:</p>
<blockquote>
<p><code>data.table</code> automatically switches to single threaded mode upon fork (the mechanism used by <code><a href="https://rdrr.io/r/parallel/mclapply.html">parallel::mclapply</a></code> and the <code>foreach</code> package). Otherwise, nested parallelism would very likely overload your CPUs and result in much slower execution. As data.table becomes more parallel internally, we expect explicit user parallelism to be needed less often…</p>
</blockquote>
<p>A final point on this topic, riffing off the quoted text, is that packages like <strong>data.table</strong> implement their parallel operations at the source-code level, i.e. in C(++) and other compiled languages. So they are likely to be more efficient than the equivalent explicit parallel calls that you might make. It’s not that you can’t combine, say, <strong>future</strong> and <strong>data.table</strong> (we both do this often). But you should know that trying to do better than the latter’s default operations may be a fool’s errand.</p>
</div>
</div>
<div id="miscellaneous" class="section level2">
<h2>
<span class="header-section-number">4.7</span> Miscellaneous<a class="anchor" aria-label="anchor" href="#miscellaneous"><i class="fas fa-link"></i></a>
</h2>
<div id="when-should-i-go-parallel" class="section level3">
<h3>
<span class="header-section-number">4.7.1</span> When should I go parallel?<a class="anchor" aria-label="anchor" href="#when-should-i-go-parallel"><i class="fas fa-link"></i></a>
</h3>
<p>The short answer is that you want to invoke the multicore option whenever you are faced with a so-called “<a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel">embarrassingly parallel</a>” problem. You can click on that link for a longer description, but the key idea is that these computational problems are easy to break up into smaller chunks. You likely have such a case if the potential code chunks are independent and do not need to communicate in any way. Classic examples include bootstrapping (since each regression or resampling iteration is drawn independently) and Markov chain Monte Carlo (i.e. <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">MCMC</a>).</p>
<p>Having said that, there are limitations to the gains that can be had from parallelization. Most obviously, there is the <a href="https://en.wikipedia.org/wiki/Overhead_(computing)">computational overhead</a> associated with splitting up the problem, tracking the individual nodes, and then bringing everything back into a single result. This can be regarded as an issue largely affecting shorter and smaller computations. In other words, the overhead component of the problem tends to diminish in relative size as the overall computation time increases.</p>
<p>On the opposite end of the spectrum, there is <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl’s law</a> (generalised as <a href="https://en.wikipedia.org/wiki/Gustafson%27s_law">Gustafson’s law</a>). This formalises the intuitive idea that there are diminishing returns to parallelization, depending on the proportion of your code that can be run in parallel. A case in point is Bayesian MCMC routines, which typically include a fixed “burn-in” period regardless of how many parallel chains are being run in parallel.</p>
<div class="inline-figure"><img src="parallel_files/figure-html/amdahl-1.png" width="672" style="display: block; margin: auto;"></div>
</div>
<div id="how-many-cores-should-i-use" class="section level3">
<h3>
<span class="header-section-number">4.7.2</span> How many cores should I use?<a class="anchor" aria-label="anchor" href="#how-many-cores-should-i-use"><i class="fas fa-link"></i></a>
</h3>
<p>If you look this question up online, you’ll find that most people recommend using <code>detectCores()-1</code>. This advice stems from the idea that you probably want to reserve one core for other tasks, such as running your web browser or word processor. While we don’t disagree, both of us typically use <em>all</em> available cores for our parallel computations. For one thing, we do most of our heavy computational work on a dedicated work station or in the cloud. Keeping some computational power in reserve doesn’t make sense in these cases. Second, when we are working locally, we’ve gotten into the habit of closing all other applications while a parallel function is running. Your mileage may vary, though. (And remember the possible diminishing returns brought on by Amdahl’s law). FWIW, calling <code>plan(multisession)</code> or <code>plan(multicore)</code> automatically default to using all your cores. You can change that by running, say, <code>plan(multisession(workers = detectCores()-1))</code>.</p>
</div>
<div id="fault-tolerance-error-catching-caching-etc." class="section level3">
<h3>
<span class="header-section-number">4.7.3</span> Fault tolerance (error catching, caching, etc.)<a class="anchor" aria-label="anchor" href="#fault-tolerance-error-catching-caching-etc."><i class="fas fa-link"></i></a>
</h3>
<p>One of the worst thing about parallel computation is that it is very sensitive to failure in any one of its nodes. An especially frustrating example is the tendency of parallel functions to ignore/hide critical errors up until the very end when they are supposed to return output. (“Oh, so you encountered a critical error several hours ago, but just decided to continue for fun anyway? Thanks!”) Luckily, all of the defensive programming tools that we practiced in Chapter <a href="funcs-adv.html#funcs-adv">3</a> — catching user errors and caching intermediate results — carry over perfectly to their parallel equivalents. Just make sure that you use a persistent cache.</p>
<blockquote>
<p><strong>Challenge:</strong> Prove this to yourself by running a parallel version of the cached iteration that we practiced in Section <a href="funcs-adv.html#caching-memoisation">3.4</a>. Specifically, you should recreate the <code>mem_square_verbose()</code> function from that section, which in turn relies on the <code>mem_square_persistent()</code> function.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;To clarify: The verbose option simply provides helpful real-time feedback to us. However, the underlying persistent cache location — provided in this case by &lt;code&gt;mem_square_persistent()&lt;/code&gt; — is necessary whenever you want to use a memoised function in the futures framework.&lt;/p&gt;"><sup>26</sup></a> You should then be able to run <code>future_map_dfr(1:10, mem_square_verbose)</code> and it will automatically return the previously cached results. After that, try <code>future_map_dfr(1:24, mem_square_verbose)</code> and see what happens.</p>
</blockquote>
</div>
<div id="random-number-generation" class="section level3">
<h3>
<span class="header-section-number">4.7.4</span> Random number generation<a class="anchor" aria-label="anchor" href="#random-number-generation"><i class="fas fa-link"></i></a>
</h3>
<p>Random number generation (RNG) can become problematic in parallel computations (whether trying to ensure the same of different RNG across processes). R has various safeguards against this and future <a href="https://www.jottr.org/2017/02/19/future-rng/">automatically handles</a> RNG via the <code>future.seed</code> argument. We saw an explicit example of this in Example 2 above.</p>
</div>
<div id="parallel-regression" class="section level3">
<h3>
<span class="header-section-number">4.7.5</span> Parallel regression<a class="anchor" aria-label="anchor" href="#parallel-regression"><i class="fas fa-link"></i></a>
</h3>
<p>A number of regression packages in R are optimized to run in parallel. For example, the superb <strong>fixest</strong> package (<a href="https://lrberge.github.io/fixest/">link</a>) that we saw in the chapter on regression analysis will automatically invoke multicore capabilities when fitting high dimensional fixed effects models. The many Bayesian packages in R are also all capable of — and, indeed, expected to — fit regression models by running their MCMC chains in parallel. For example, <strong>RStan</strong> (<a href="https://mc-stan.org/rstan/articles/rstan.html#running-multiple-chains-in-parallel">link</a>). Finally, you may be interested in the <strong>partools</strong> package (<a href="https://cran.r-project.org/web/packages/partools/index.html">link</a>), which provides convenient aliases for running a variety of statistical models and algorithms in parallel.</p>
</div>
<div id="cpus-vs-gpus" class="section level3">
<h3>
<span class="header-section-number">4.7.6</span> CPUs vs GPUs<a class="anchor" aria-label="anchor" href="#cpus-vs-gpus"><i class="fas fa-link"></i></a>
</h3>
<p>Graphical Processing Units, or GPUs, are specialised chipsets that were originally built to perform the heavy lifting associated with rendering graphics. It’s important to realise that not all computers have GPUs. Most laptops come with so-called <a href="https://www.laptopmag.com/articles/intel-hd-graphics-comparison">integrated graphics</a>, which basically means that the same processor is performing both regular and graphic-rendering tasks. However, gaming and other high-end laptops (and many desktop computers) include a dedicated GPU card. For example, the Dell Precision 5530 that Grant is writing this book on has a <a href="https://wiki.archlinux.org/index.php/hybrid_graphics">hybrid graphics</a> setup with two cards: 1) an integrated Intel GPU (UHD 630) and 2) a discrete NVIDIA Quadro P2000.</p>
<p>So why are we telling you this? Well, it turns out that GPUs also excel at non-graphic computation tasks. The same processing power needed to perform the millions of parallel calculations for rendering 3-D games or architectural software, can be put to use on scientific problems. How exactly this was discovered involves an interesting backstory of supercomputers being built with Playstations. (Look it up.) But the short version is that modern GPUs comprise <em>thousands</em> of cores that can be run in parallel. Or, as our colleague <a href="http://econevans.com/">David Evans</a> once memorably described it to me: “GPUs are basically just really, really good at doing linear algebra.”</p>
<p>Still, that’s about as much as we want to say about GPUs for now. Installing and maintaining a working GPU setup for scientific purposes is a much more complex task. (And, frankly, overkill for the vast majority of econometric or data science needs.) We will revisit the topic when we get to the machine learning section of the book.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Advanced machine learning techniques like &lt;a href="https://blog.rstudio.com/2018/09/12/getting-started-with-deep-learning-in-r/"&gt;deep learning&lt;/a&gt; are particularly performance-dependent on GPUs.&lt;/p&gt;'><sup>27</sup></a> Thus, and while the general concepts carry over, everything that we’ve covered in this chapter is limited to CPUs.</p>
</div>
<div id="monitoring-multicore-performance" class="section level3">
<h3>
<span class="header-section-number">4.7.7</span> Monitoring multicore performance<a class="anchor" aria-label="anchor" href="#monitoring-multicore-performance"><i class="fas fa-link"></i></a>
</h3>
<p>Bash-compatible shells should come with the built-in <code>top</code> command, which provides a real-time view of running processes and resource consumption. (Pro-tip: Hit “1” to view processes across individual cores and “q” to quit.) An enhanced alternative that we really like and use all the time is <a href="https://hisham.hm/htop/"><strong>htop</strong></a>, which is available on both Linux and Mac. (Windows users can install <code>htop</code> on the WSL that we covered way back in the <a href="https://raw.githack.com/uo-ec607/lectures/master/03-shell/03-shell.html#windows">shell lecture</a>.). It’s entirely up to you whether you want to install it. Your operating system almost certainly provides built-in tools for monitoring processes and resource usage (e.g. <a href="https://wiki.gnome.org/Apps/SystemMonitor">System Monitor</a>). However, we wanted to flag <code>htop</code> before we get to the big data section of the course. We’ll all be connecting to remote Linux servers at that point and a shell-based (i.e. non-GUI) process monitor will prove very handy for tracking resource use.</p>
</div>
</div>
<div id="further-resources-2" class="section level2">
<h2>
<span class="header-section-number">4.8</span> Further resources<a class="anchor" aria-label="anchor" href="#further-resources-2"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Dirk Eddelbuettel provides the authoritative reference on this topic in his review paper, <a href="https://doi.org/10.1002/wics.1515"><em>Parallel Computing With R: A Brief Review</em></a> (<a href="https://arxiv.org/pdf/1912.11144">pre-print</a>).</li>
<li>Beyond Dirk’s article, we’d argue that the starting point for further reading should be the <strong>future</strong> vignettes (<a href="https://cran.r-project.org/web/packages/future/vignettes/future-1-overview.html">one</a>, <a href="https://cran.r-project.org/web/packages/future/vignettes/future-2-output.html">two</a>, <a href="https://cran.r-project.org/web/packages/future/vignettes/future-3-topologies.html">three</a>, <a href="https://cran.r-project.org/web/packages/future/vignettes/future-4-issues.html">four</a>, <a href="https://cran.r-project.org/web/packages/future/vignettes/future-5-startup.html">five</a>). There’s a lot in there, so feel free to pick and choose.</li>
<li>Similarly, the <a href="https://davisvaughan.github.io/furrr/index.html">furrr package vignette</a> is very informative (and concise).</li>
<li>The <a href="https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf">parallel package vignette</a> provides a very good overview, not only its own purpose, but of parallel programming in general. Particular attention is paid to the steps needed to ensure a stable R environment (e.g. across operating systems).</li>
<li>Finally, there a number of resources online that detail older parallel programming methods in R (<code>foreach</code>, <code>mclapply</code>, <code>parLapply</code> <code>snow</code>, etc.). While these methods have clearly been superseded by the future package ecosystem in our view, there is still a lot of valuable information to be gleaned from understanding them. Two of our favourite resources in this regard are: <a href="http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/">How-to go parallel in R</a> (Max Gordon) and <a href="https://github.com/ljdursi/beyond-single-core-R">Beyond Single-Core R</a> (Jonathan Dursi).</li>
</ul>
</div>
</div>




  <div class="chapter-nav">
<div class="prev"><a href="funcs-adv.html"><span class="header-section-number">3</span> Functions: Advanced concepts</a></div>
<div class="next"><a href="spatial-analysis.html"><span class="header-section-number">5</span> Spatial analysis</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#parallel"><span class="header-section-number">4</span> Parallel programming</a></li>
<li>
<a class="nav-link" href="#software-requirements-2"><span class="header-section-number">4.1</span> Software requirements</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#r-packages-2"><span class="header-section-number">4.1.1</span> R packages</a></li></ul>
</li>
<li><a class="nav-link" href="#prologue"><span class="header-section-number">4.2</span> Prologue</a></li>
<li><a class="nav-link" href="#example-1-slow_square"><span class="header-section-number">4.3</span> Example 1: slow_square</a></li>
<li>
<a class="nav-link" href="#example-2-bootstrapping"><span class="header-section-number">4.4</span> Example 2: Bootstrapping</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#serial-implementation-for-comparison"><span class="header-section-number">4.4.1</span> Serial implementation (for comparison)</a></li>
<li><a class="nav-link" href="#parallel-implemention-using-the-future-ecosystem"><span class="header-section-number">4.4.2</span> Parallel implemention using the future ecosystem</a></li>
<li><a class="nav-link" href="#results"><span class="header-section-number">4.4.3</span> Results</a></li>
<li><a class="nav-link" href="#other-parallel-options"><span class="header-section-number">4.4.4</span> Other parallel options</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#general-parallel-programming-topics"><span class="header-section-number">4.5</span> General parallel programming topics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#terminology"><span class="header-section-number">4.5.1</span> Terminology</a></li>
<li><a class="nav-link" href="#a-bit-more-about-logical-cores-and-hyperthreading"><span class="header-section-number">4.5.2</span> A bit more about logical cores and hyperthreading</a></li>
<li><a class="nav-link" href="#forking-vs-sockets"><span class="header-section-number">4.5.3</span> Forking vs Sockets</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#explicit-vs-implicit-parallelization"><span class="header-section-number">4.6</span> Explicit vs implicit parallelization</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#blaslapack"><span class="header-section-number">4.6.1</span> BLAS/LAPACK</a></li>
<li><a class="nav-link" href="#beware-resource-competition"><span class="header-section-number">4.6.2</span> Beware resource competition</a></li>
<li><a class="nav-link" href="#library-source-code"><span class="header-section-number">4.6.3</span> Library source code</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#miscellaneous"><span class="header-section-number">4.7</span> Miscellaneous</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#when-should-i-go-parallel"><span class="header-section-number">4.7.1</span> When should I go parallel?</a></li>
<li><a class="nav-link" href="#how-many-cores-should-i-use"><span class="header-section-number">4.7.2</span> How many cores should I use?</a></li>
<li><a class="nav-link" href="#fault-tolerance-error-catching-caching-etc."><span class="header-section-number">4.7.3</span> Fault tolerance (error catching, caching, etc.)</a></li>
<li><a class="nav-link" href="#random-number-generation"><span class="header-section-number">4.7.4</span> Random number generation</a></li>
<li><a class="nav-link" href="#parallel-regression"><span class="header-section-number">4.7.5</span> Parallel regression</a></li>
<li><a class="nav-link" href="#cpus-vs-gpus"><span class="header-section-number">4.7.6</span> CPUs vs GPUs</a></li>
<li><a class="nav-link" href="#monitoring-multicore-performance"><span class="header-section-number">4.7.7</span> Monitoring multicore performance</a></li>
</ul>
</li>
<li><a class="nav-link" href="#further-resources-2"><span class="header-section-number">4.8</span> Further resources</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/grantmcdermott/ds4e/blob/master/parallel.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/grantmcdermott/ds4e/edit/master/parallel.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Data Science for Economists and Other Animals</strong>" was written by Grant McDermott and Ed Rubin. It was last built on 2021-08-03.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
